1)EDA:
Origial data is analysed, column wise and missing values and noise are corrected , then the data is analysed again.
The text is cleaned by removing mentions , instagram tags and twitter tags .
The tidy test is then made into corpus using nltk , then its made into embedding vectors.
Original_author is then made into dummies(onehot encoded)and along with number of retweets its is concatenated with embedding data.
This is made into dataframe and downloaded as csv
the same process is done for test data and downloaded as csv
the label data is also downloaded as csv 

2)lgbm2:
The downloaded csv files from eda are loaded using pandas
train data is split into train and validation  
and using lightgbm classifier , model is trained (cv is internally used on train data)
model is validated with validation data and confusion matrix is observed,
then the model is used to predict labels on test data.
the predicted test labels is made as a dataframe and analysed and downloaded as csv file

the model is saved  as pickle file using joblib. 
