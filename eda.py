# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-jChe6RDWaMa80SA1QKzBwfDPygJQxxY
"""

# Commented out IPython magic to ensure Python compatibility.
# Import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
import nltk 
import string
import re
# %matplotlib inline

df1=pd.read_csv("train.csv")
print(df1.shape)
df1.head()

df1.info()

sns.countplot(x='original_author', hue="sentiment_class", data = df1)

sns.countplot( x="sentiment_class", data = df1)

print(df1.lang.nunique())
df1.lang.unique()

print(df1.original_author.nunique())
df1.original_author.unique()

df1.isnull().sum()

y=df1['sentiment_class']
y.shape

"""***   ***

***retweet_count***
"""

print(df1.retweet_count[0])
isinstance(df1.retweet_count[1], (str))

print(df1.retweet_count.nunique())
df1.retweet_count.unique()

rc=df1['retweet_count']
rc

def convertible(v):
    try:
        int(v)
        return True
    except (TypeError, ValueError):
        return False


res = [int(ele) if convertible(ele)  else int(0) for ele in rc] 
res

res=pd.DataFrame(res)
res.head()

print(res[0].nunique())
res[0].unique()

df1['retweet_count']=res[0]
df1['retweet_count'].unique()

"""**   **

***Orginal_author***
"""

print(df1.original_author.nunique())
df1.original_author.unique()

oa=df1['original_author']
oa

def convertibles(v):
    try:
        float(v)
        return True
    except (TypeError, ValueError):
        return False


res2 = [str('unkn') if convertibles(ele) else ele  for ele in oa] 
res2

res2=pd.DataFrame(res2)

print(res2[0].nunique())
res2[0].unique()

df1['original_author']=res2[0]
df1['original_author'].nunique()

"""**   **

**test_data**
"""

df2=pd.read_csv("test.csv")
print(df2.shape)
df2.head()

print(df2.retweet_count.nunique())
df2.retweet_count.unique()

rct=df2['retweet_count']
rct

def convertible(v):
    try:
        int(v)
        return True
    except (TypeError, ValueError):
        return False


rest = [int(ele) if convertible(ele)  else int(0) for ele in rct] 
rest

rest=pd.DataFrame(rest)
rest.head()

print(rest[0].nunique())
rest[0].unique()

df2['retweet_count']=rest[0]
df2['retweet_count'].unique()

"""**    **"""

print(df2.original_author.nunique())
#df2.original_author.unique()

oat=df2['original_author']
oat

def convertibles(v):
    try:
        float(v)
        return True
    except (TypeError, ValueError):
        return False


rest2 = [str('unkn') if convertibles(ele) else ele  for ele in oat] 
rest2

rest2=pd.DataFrame(rest2)

print(rest2[0].nunique())
#rest2[0].unique()

df2['original_author']=rest2[0]
df2['original_author'].nunique()



"""**    **"""

print(df1.head())
df1.info()

print(df2.head())
df2.info()

np.intersect1d([1,2,3,1,4,3,5,7], [1,3,4,6])        #testing

com=np.intersect1d(df1.original_author, df2.original_author)
com.shape         #246 common in(1528-736)

"""**   **

***tweets***
"""

x1=df1['original_text']
print(x1.shape)
x1.head

x2=df2['original_text']
print(x2.shape)
x2.tail()

x=pd.concat([x1,x2],axis=0)
x=pd.DataFrame(x)
x.reset_index(inplace=True)
x=x['original_text']
x=pd.DataFrame(x)
print(x.shape)
x.tail()

"""**Data cleaning**"""

def remove_pattern1(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
        input_txt = re.sub(i, '', input_txt)
        
    return input_txt

# remove twitter handles (@user)
x['tidy_tweet'] = np.vectorize(remove_pattern1)(x['original_text'], "@[\w]*")

x.head()



"""**removing links and mentions**"""

def remove_pattern(pattern):
  for i in range(x['tidy_tweet'].shape[0]):
    twt=x['tidy_tweet'][i]
    res=twt.split(pattern, maxsplit=1) 
    x['tidy_tweet'][i]=res[0]

remove_pattern('http')

remove_pattern('pic.twitter')



x.head

x['original_text'][7]

x['tidy_tweet'][7]

"""**   **

**NLTK**
"""

import re
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in range(0, 4622):
    review = re.sub('[^a-zA-Z]', ' ',x['tidy_tweet'][i])
    review = review.lower()
    review = review.split()
    ps = PorterStemmer()
    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
    review = ' '.join(review)
    corpus.append(review)

corpus[2]

x['tidy_tweet'][2]

"""**   **

**Embedding Representation**
"""

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from keras import utils
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense 
from tensorflow.keras.layers import Dropout

voc_size=7500

onehot_repr=[one_hot(words,voc_size)for words in corpus]

sent_length=35
embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)
print(embedded_docs)

embedded_docs[10]



import numpy as np
X_=np.array(embedded_docs)
print(X_.shape)

"""**   **"""

f1=df1[['retweet_count','original_author']]
print(f1.shape)
f1.head()

f2=df2[['retweet_count','original_author']]
print(f2.shape)
f2.head()

f=pd.concat([f1,f2],axis=0)
f.reset_index(inplace=True)
f=f.drop("index",axis=1)
print(f.shape)
f.head()

f=pd.get_dummies(f, prefix=['original_author'])
print(f.shape)

X_pd=pd.DataFrame(X_)
X=pd.concat([X_pd,f],axis=1)
X.reset_index(inplace=True)
X=X.drop("index",axis=1)
print(X.shape)
X.tail()

xtrain=X.iloc[:3235,: ].values
xtest=X.iloc[3235:,: ].values
print(xtrain.shape)
xtest.shape

y=df1['sentiment_class']
y.shape

xtrain=pd.DataFrame(xtrain)
y=pd.DataFrame(y)
xtest=pd.DataFrame(xtest)

